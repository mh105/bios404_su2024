{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author: Tom Stone <tomstone@stanford.edu>\n",
    "# author: Proloy Das <email:proloyd94@gmail.com>\n",
    "# License: BSD (3-clause)\n",
    "%matplotlib widget\n",
    "%matplotlib widget\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import signal, fft\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from lab1_utils import *\n",
    "\n",
    "# rcParams are set to make the plots pretty (i.e., publication ready).\n",
    "pyplot.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"Helvetica\",\n",
    "    \"figure.constrained_layout.use\": True,\n",
    "    \"savefig.dpi\": 300      # very imp for publications\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a random number generator, and seed it (the argument) to make the random number generation reproducible. \n",
    "That is, the numbers generarted will be 'random' in the probabilty measure sense, but they will be the same everytime you run the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a white noise signal\n",
    "Create n=1024 time-samples of a Gaussian white noise process.\n",
    "$$𝔼[𝑤_𝑘]=𝜇$$\n",
    "$$Var(𝑤_𝑘 )=𝜎^2$$\n",
    "$$Cov(𝑤_𝑘, 𝑤_𝑙 )=0  \\text{ for } 𝑘≠𝑙$$\n",
    "Assume $\\mu = 0$. Hint: use `rng.normal()`\n",
    "\n",
    "Also create the time indices corresponding the white noise sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = rng.normal(size=1024)\n",
    "time_indices = np.arange(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the white noise againt time using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax = pyplot.subplots(figsize=(8, 2))\n",
    "ax.plot(time_indices, wn, linewidth=0.5)\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$w_t$')\n",
    "ax.set_ylim([-4, 4])\n",
    "ax.set_title('White noise process')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the autocovaraince sequence from the realization that we generated.\n",
    "$$\\hat{s}_k = \\frac{1}{N}\\sum_{i=1}^{N-\\vert k\\vert}(x_i-\\hat{\\mu})(x_{i+\\vert k \\vert} -\\hat{\\mu})$$\n",
    "For that write a funtion with `compute_autocovaraince(signal, max_lag)` that takes in the signal and the maximum lag upto which it needs to compute the autocovaraince sequence as inputs, and returns the autocovaraince, and the associated time indices as output.\n",
    "\n",
    "Note: This problem can be solved very easily with only a few line of code using `signal.correlate` funtion as instructed bewlow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_autocovaraince(x, max_lag=512):\n",
    "    \"\"\"Computes autocovarinane upto a given lag.\n",
    "    \n",
    "    Parameters:\n",
    "        x:\n",
    "            the signal\n",
    "        max_lag: \n",
    "            maximum lag to consider for autocovaraince sequenc\n",
    "    Returns:\n",
    "        acov:\n",
    "            the sample autocovariance, normalized by the number of samples.\n",
    "        time_indices:\n",
    "         integer shifts, i.e the x-axis for the autocovariance plot.\n",
    "    \"\"\"\n",
    "    assert max_lag > 0, f\"max_lag needs to be >0, received {max_lag}\"\n",
    "    n = wn.shape[-1] # length of time series\n",
    "    assert max_lag < n, f\"max_lag needs to be < signal length ({n}) , received {max_lag}\"\n",
    "    # Compute the mean, `mu`\n",
    "    mu = x.sum(axis=-1) / n\n",
    "\n",
    "    # Remove the mean from the sequence\n",
    "    x = x - mu\n",
    "\n",
    "    # Use `signal.correlate` function to compute the acov sequence\n",
    "    # See https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.correlate.html\n",
    "    # Use mode='full' which returns the full result, i.e. lag -n+1 to lag n-1\n",
    "    acov = signal.correlate(x, x, mode='full', method='fft') / n\n",
    "    # Create an index array corrsponding to the correlation values using `numpy.arange``\n",
    "    indices = np.arange(-n+1, n)\n",
    "\n",
    "    # Create a boolean selection to extract indices within range[-max_lag, max_lag].\n",
    "    # Use `numpy.logical_and()`\n",
    "    selection = np.logical_and(indices < max_lag+1, indices > -max_lag-1)\n",
    "    return acov[selection], indices[selection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the function to compute sample autocovariance sequnce, and compare with the true autocovaraince sequence using a plot.\n",
    "$$s_k =\\begin{cases}\n",
    "        1 \\text{, } k=0 \\\\\n",
    "        0 \\text{, otherwise}\n",
    "        \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampe autocorrelation\n",
    "max_lag = wn.shape[-1] - 1\n",
    "sample_acov, lags = compute_autocovaraince(wn, max_lag)\n",
    "\n",
    "# True autocovaraince\n",
    "true_acov = np.zeros_like(sample_acov)\n",
    "true_acov[max_lag] = 1\n",
    "\n",
    "fig1, ax = pyplot.subplots(figsize=(8, 2))\n",
    "ax.plot(lags, sample_acov, linewidth=1, color='r', label='Sample')\n",
    "ax.plot(lags, true_acov, linewidth=1, color='b', label='True')\n",
    "ax.set_ylim([-0.2, 1.2])\n",
    "ax.legend()\n",
    "_ = ax.set_title('Autocorrelation sequence plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to compute the periodogram. Create a function with signature `compute_periodogram(x)` that computes the autocovaraince sequence, and takes its Fourier transform.\n",
    "$$\\hat{S}(f) = T \\sum_{-N+1}^{N-1} \\hat{s}_k\\exp(-i2\\pi k fT)$$\n",
    "The function shall return the power spectral density (PSD) estimate, and associated (normalized) frequency points.\n",
    "\n",
    "Note: Use `scipy.fft.fft()` function perform the transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_xx_est = np.abs(fft.fft(sample_acov))\n",
    "S_xx_true = np.abs(fft.fft(true_acov))\n",
    "freqs = np.linspace(0, 1, num=len(lags))\n",
    "\n",
    "# Periodogram plot\n",
    "fig1, ax = pyplot.subplots(figsize=(5, 2))\n",
    "ax.plot(freqs, 10*np.log10(S_xx_est), linewidth=1, color='r', label='Sample')\n",
    "ax.plot(freqs, 10*np.log10(S_xx_true), linewidth=1, color='b', label='True')\n",
    "ax.set_ylim([-30, 20])\n",
    "ax.set_xlim([0., 0.5])\n",
    "ax.legend()\n",
    "_ = ax.set_title('Periodogram plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a function with signature `compute_periodogram(x)` that uses the following formulae:\n",
    "$$\\hat{S}(f) = \\frac{T}{N} \\left\\vert\\sum_{k=0}^{N-1} x_k\\exp(-i2\\pi k fT)\\right\\vert^2$$\n",
    "The function shall return the power spectral density (PSD) estimate, and associated (normalized) frequency points.\n",
    "\n",
    "Note: Use `scipy.fft.fft()` function perform the transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_periodogram(x):\n",
    "    \"\"\"Compute periodogram using fft directly on the signal.\n",
    "    Parameters:\n",
    "        x:\n",
    "            the signal\n",
    "        max_lag: \n",
    "            maximum lag to consider for autocovaraince sequenc\n",
    "    Returns:\n",
    "        S_xx:\n",
    "            periodogram.\n",
    "        freqs:\n",
    "         associated frequency (normalized) points.\n",
    "    \"\"\"\n",
    "    n = x.shape[-1]\n",
    "    # Compute the mean\n",
    "    mu = x.sum() / n\n",
    "    # remove the mean from the signal\n",
    "    x = x - mu\n",
    "    \n",
    "    # Now use the `fft.fft()` funtion to perform the computation.\n",
    "    S_xx = np.abs(fft.fft(x)) ** 2 / n\n",
    "    freqs = np.linspace(0., 1, num=n)\n",
    "    return S_xx, freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the output of the `compute_periodogram()` to the previously computed periodograms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_xx, freqs = compute_periodogram(wn)\n",
    "ax.plot(freqs, 10*np.log10(S_xx), linewidth=1, color='g', label='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations?\n",
    "\n",
    "1. \n",
    "2.\n",
    "...\n",
    "\n",
    "We will use the `compute periodogram` function extensively for the rest of the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before moving further in this notebook, we will take a look at the random processes that are more structured than white noise. \n",
    "Lets jump to the AR(2) notebook, and run the same analysis. But, for running same analysis will require us to copy the funtions we just wrote to AR(2) notebook. We will approach this copying a pythonic way. Instead of copying the functions to other notebooks, we will copy them to another python scipt named lab1_utils.py.\n",
    "\n",
    "You might ask *Why?*. The reason behind this is that once the functions are there, we simply need to import the functions as required, e.g.,\n",
    "```from lab_utils import compute_autocovariance```\n",
    "instead of copying the code anymore. \n",
    "\n",
    "Less code duplication is your friend for combatting against grave mistakes in data analysis. It makes your analysis managable from one place, thus making mudane code verification work very quick."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
